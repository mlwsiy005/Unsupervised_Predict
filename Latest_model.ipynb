{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pip install comet_ml","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import comet_ml in the top of your file\n#import comet_ml\n#from comet_ml import Experiment\n\n#experiment = Experiment(api_key=\"PXaqoLTlHzWF85kimEv0zHJPz\",\n #                       project_name=\"edsa-movie-recommender\", workspace=\"lejone\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# utilities\nimport numpy as np\nimport pandas as pd\n\n#plotting\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nplt.style.use('ggplot')\n%matplotlib inline\nsns.set()\n\nfrom sklearn.metrics import mean_squared_error\n\nimport surprise\n# Import libraries from Surprise package\nfrom surprise import Reader, Dataset, SVD\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Context manager allows logging og parameters\n\n#experiment.context_manager(\"error\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# URL where experiments can be found\n\n#experiment.url","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(r\"../input/edsa-recommender-system-predict/train.csv\")\ntest_df =  pd.read_csv(r\"../input/edsa-recommender-system-predict/test.csv\")\n#scores = pd.read_csv(r\"../input/edsa-recommender-system-predict/genome_scores.csv\")\n#tags = pd.read_csv(r\"../input/edsa-recommender-system-predict/genome_tags.csv\")\n#imbd = pd.read_csv(r\"../input/edsa-recommender-system-predict/imdb_data.csv\")\n#links = pd.read_csv(r\"../input/edsa-recommender-system-predict/links.csv\")\n#movies = pd.read_csv(r\"../input/edsa-recommender-system-predict/movies.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Model-Based Collaborative Filtering"},{"metadata":{},"cell_type":"markdown","source":"Model-based Collaborative Filtering is based on matrix factorization (MF) which is widely used for recommender systems where it can deal better with scalability and sparsity than Memory-based Collaborative Filtering:\n\n- The goal of MF is to learn the latent preferences of users and the latent attributes of items from known ratings (learn features that describe the characteristics of ratings) to then predict the unknown ratings through the dot product of the latent features of users and items.\n- When you have a very sparse matrix, with a lot of dimensions, by doing matrix factorization, you can restructure the user-item matrix into low-rank structure, and you can represent the matrix by the multiplication of two low-rank matrices, where the rows contain the latent vector."},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Decomposition (SVD)"},{"metadata":{},"cell_type":"markdown","source":"A well-known matrix factorization method is Singular value decomposition (SVD). SVD is an algorithm that decomposes a matrix $A$ into the best lower rank (i.e. smaller/simpler) approximation of the original matrix $A$. Mathematically, it decomposes A into a two unitary matrices and a diagonal matrix:"},{"metadata":{},"cell_type":"markdown","source":"# Model EvaluationÂ¶"},{"metadata":{},"cell_type":"markdown","source":"Since the data is very big (10 million rows), the train data will be sampled in batches to see how much RMSE score it produces. The data will be trained on three samples of 10 %, 50 % and 80 %. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_sample_1 = train_df.sample(frac =.1)\n#train_sample_5 = train_df.sample(frac =.5)\ntrain_sample_8 = train_df.sample(frac =.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the SVD algorithm.\nalgo_svd = SVD()\n\n# Load Reader library\nreader = Reader()\n\n# Load ratings dataset with Dataset library\n#data_1 = Dataset.load_from_df(train_sample_1[['userId', 'movieId', 'rating']], reader)\n#data_5 = Dataset.load_from_df(train_sample_5[['userId', 'movieId', 'rating']], reader)\ndata_8 = Dataset.load_from_df(train_sample_8[['userId', 'movieId', 'rating']], reader)\n#data_100 = Dataset.load_from_df(train_sample_100[['userId', 'movieId', 'rating']], reader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run 3-fold cross-validation for fourth sample and print results for SVD.\nCV_8 = surprise.model_selection.cross_validate(algo_svd, data_8, measures=['RMSE'], cv=3, verbose=True, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compare Perfomance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate RMSE and Test Times\n\nrmse_list = [0.9210, 0.8851, 0.8598]\nfit_list = [88.71, 451.35, 721.30]\ntest_list = [6.36, 34.83, 57.10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot of Run times\nlabels = ['Sample 10 %', 'Sample 50 %', 'Sample 80 %', 'Sample 100 %']\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.15  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width/2, fit_list, width, label='Train')\nrects2 = ax.bar(x + width/2, test_list, width, label='Test')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Time (sec)')\nax.set_title('Time To Run Test by Sample Size')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects1)\nautolabel(rects2)\n\nfig.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot of RMSE values\n\nfig, ax = plt.subplots()\n\nax.bar(x, rmse_list, align='center')\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('RMSE')\nax.set_title('RMSE Over Different Samples ')\nax.set_xticks(x)\nax.set_xticklabels(labels)\n\nplt.xticks(x, labels) #Replace default x-ticks with xs, then replace xs with labels\nplt.yticks(rmse_list)\n\nfig.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots above, the following observations can be made:\n\n- The time to train the model increases with the amount of data used to train it. \n- The RMSE score decreases with more samples of the train dataset used."},{"metadata":{},"cell_type":"markdown","source":"The model will be fit using the best performing sample (all the data), and generate predictions for this model"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = data_100.build_full_trainset()\nalgo_svd.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is for Comet purposes\n#pre_comet = []\n\n#for _, row in train_sample.iterrows():\n #   x_unseen = algo_svd.predict(uid=row['userId'],iid=row['movieId'])\n  #  pred = x_unseen[3]\n   # pre_comet.append(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the predictions\npre = []\n\nfor _, row in test_df.iterrows():\n    x_unseen = algo_svd.predict(uid=row['userId'],iid=row['movieId'])\n    pred = x_unseen[3]\n    pre.append(pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Format the test data for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Id'] = test_df['userId'].map(str)+ \"_\" +test_df['movieId'].map(str)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sellecting the index of the test dataframe\nfinal_test= test_df[\"Id\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating the submission Dataframe\nFinal_Table = {'Id': final_test, 'rating':np.round(pre, 1)}\nsubmission = pd.DataFrame(data=Final_Table)\nsubmission = submission[['Id', 'rating']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"Submission.csv\",index  = False) #wrting csv file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Log metrics\n\n#experiment = Experiment(api_key=\"PXaqoLTlHzWF85kimEv0zHJPz\")\n#with experiment.context_manager(\"error\"):\n  #algo_svd.fit(trainset)\n # rmse_svd = mean_squared_error(train_sample['rating'],pre_comet, squared=False)\n  # returns the validation accuracy\n  #experiment.log_metric(\"rmse\", rmse_svd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# End the experiment\n\n#experiment.end()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display comet experiment\n\n#experiment.display()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import pickle\n#model_save_path = \"SVD.pkl\"\n#with open(model_save_path,'wb') as file:\n #   pickle.dump(svd,file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from IPython.display import FileLink\n#FileLink(r'SVD.pkl')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}